{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edbe0b4",
   "metadata": {},
   "source": [
    "# Connect with Supabase DB with SQL alchemy\n",
    "\n",
    "## Create a Supabase DB\n",
    "* Go to https://app.supabase.com/\n",
    "* Create an account\n",
    "* create a project\n",
    "* create a table in the DB with some data.\n",
    "\n",
    "        create table test_table (\n",
    "          id bigint generated by default as identity primary key,\n",
    "          name text\n",
    "        );\n",
    "        insert into test_table\n",
    "        select 1, 'Saeed';\n",
    "        commit;\n",
    "    \n",
    "    \n",
    "* Save project password & reference_id (from settings)\n",
    "\n",
    "## Download required packages\n",
    "* install sqlalchemy \n",
    "\n",
    "        pip install SQLAlchemy\n",
    "        \n",
    "* install psycopg2-binary to access the postgresdb in supabase \n",
    "\n",
    "        pip install psycopg2-binary --user\n",
    "        \n",
    "* Run the following code to test connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0947cf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://postgres:DataIntegrationFramework@db.evzquwutdsfcxeaeenqm.supabase.co:5432/postgres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/dwql3nfj2yjdrgnnddxpfr740000gn/T/ipykernel_96322/3827848375.py:10: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  table = sqlalchemy.Table(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'Saeed')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        import sqlalchemy\n",
    "\n",
    "        db_password = \"YourPass\"\n",
    "        db_reference_id = \"YourReferenceID\"\n",
    "        db_url = f'postgresql://postgres:{db_password}@db.{db_reference_id}.supabase.co:5432/postgres'\n",
    "        print(db_url)\n",
    "        engine = sqlalchemy.create_engine(db_url)\n",
    "        metadata = sqlalchemy.MetaData(bind=None)\n",
    "\n",
    "        table = sqlalchemy.Table(\n",
    "            'test_table', \n",
    "            metadata, \n",
    "            autoload=True, \n",
    "            autoload_with=engine\n",
    "        )\n",
    "\n",
    "        stmt = sqlalchemy.select([\n",
    "            table.columns.id,\n",
    "            table.columns.name\n",
    "        ])\n",
    "\n",
    "        connection = engine.connect()\n",
    "        results = connection.execute(stmt).fetchall()\n",
    "        results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636f1a1",
   "metadata": {},
   "source": [
    "# Now Access the data with Spark\n",
    "\n",
    "\n",
    "## Get the JDBC URL for supabase DB\n",
    "* Go to [Supabase](https://app.supabase.com/)\n",
    "* Go to your created project\n",
    "* Go to Project Settings > Database > Connection String\n",
    "* Select the JDBC connection string and save\n",
    "\n",
    "\n",
    "## Download Spark\n",
    "* Go to the terminal of your computer and execute the following command.\n",
    "\n",
    "        pip install pyspark\n",
    "        \n",
    "If pip command is not found, install pip following the instructions [here](https://packaging.python.org/en/latest/tutorials/installing-packages/).\n",
    "\n",
    "## Install Java\n",
    "* If you do not have java installed, install java from [here](https://www.oracle.com/java/technologies/downloads/)\n",
    "\n",
    "## Download JDBC Driver\n",
    "* Download PostgreSQL JDBC Driver (jar file) ftom [here](https://jdbc.postgresql.org/download/)\n",
    "\n",
    "## Add the driver to Spark classpath\n",
    "* Go to terminal. Execute the following command to start spark shell.\n",
    "       \n",
    "       % spark-shell\n",
    "        Setting default log level to \"WARN\".\n",
    "        To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "        Spark context Web UI available at http://192.168.0.102:4040\n",
    "        Spark context available as 'sc' (master = local[*], app id = local-1674349254218).\n",
    "        Spark session available as 'spark'.\n",
    "        Welcome to\n",
    "              ____              __\n",
    "             / __/__  ___ _____/ /__\n",
    "            _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "           /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.1\n",
    "              /_/\n",
    "\n",
    "        Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 19.0.1)\n",
    "        Type in expressions to have them evaluated.\n",
    "        Type :help for more information.\n",
    "\n",
    "        scala> \n",
    "        \n",
    "* Now add the path to the PostgreSQL JDBC driver jar file to spark Class path.\n",
    "\n",
    "        scala> :require /path/to/postgresql-42.5.1.jar\n",
    "        Added '/path/to/postgresql-42.5.1.jar' to classpath.\n",
    "        scala> import java.util.Properties\n",
    "        import java.util.Properties\n",
    "  \n",
    "## Run the following code to test the connection using pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2200446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Saeed|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import traceback\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "               .appName('Spark_jdbc_connect') \\\n",
    "               .getOrCreate()\n",
    "\n",
    "    # Read table using jdbc()\n",
    "    dataframe_postgresql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\",\"jdbc:postgresql://db.evzquwutdsfcxeaeenqm.supabase.co:5432/postgres?user=postgres&password=DataIntegrationFramework\") \\\n",
    "    .option(\"dbtable\",\"test_table\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "    \n",
    "    dataframe_postgresql.show()\n",
    "except Exception as e:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9495894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_integration_framework",
   "language": "python",
   "name": "data_integration_framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
